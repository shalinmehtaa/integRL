[project]
name = "integRL"
version = "1.0.0"
description = "Integrating math reasoning into LLMs with RL."
readme = "README.md"
requires-python = ">=3.11,<3.13"  # Python 3.13 not yet supported for some deps
dependencies = [
    "accelerate>=1.5.2",
    "alpaca-eval",
    "flash-attn==2.7.4.post1",
    "jupyter>=1.1.1",
    "math-verify[antlr4-13-2]>=0.7.0",
    "pylatexenc==2.10",
    "notebook>=7.4.2",
    "pytest>=8.3.5",
    "torch",
    "tqdm>=4.67.1",
    "transformers>=4.50.0",
    "typer>=0.15.4",
    "vllm==0.7.2",
    "wandb>=0.19.8",
    "xopen>=2.0.2",
    "ipywidgets>=8.1.7",
    "matplotlib>=3.10.5",
]

[tool.setuptools]
packages = ["integrl"]

[tool.uv]
package = true
no-build-isolation-package = [
    "flash-attn"
]

[tool.uv.sources]
alpaca-eval = { git = "https://github.com/nelson-liu/alpaca_eval.git", rev = "forward_kwargs_to_vllm" }

[tool.pytest.ini_options]
log_cli = true
log_cli_level = "WARNING"

[[tool.uv.dependency-metadata]]
name = "flash-attn"
version = "2.7.4.post1"
requires-dist = ["torch", "einops", "setuptools"]

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
]
